{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0263a1a-6475-4403-9e99-61fe619e930c",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d0bac63-fa44-428c-970e-619604beb83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb8b7f95-4de2-4b4b-9484-a1a3f0061252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61477fa-6f2f-4341-87fe-886cf12767a4",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fb2d8ba-2d68-4a37-afc1-75e2cd2c7ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = json.load(open(\"./datasets/correction_train.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b47e0e6a-b46f-493c-a17a-89e4b57e186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_text = [d[\"from\"] for d in dataset]\n",
    "merged_text = \"\\n\".join(merged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb1be5d-b0d6-490c-90b9-b2c6abec2d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./datasets/vocab_generic.txt\", \"w\") as file:\n",
    "    file.write(merged_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c6f5c2-eb71-4dd8-a954-091c26a6c584",
   "metadata": {},
   "source": [
    "# Build Fast Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2058146-96af-473b-9039-5828ca9104a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TOKENIZERS_PARALLELISM=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "535b6f0c-cc28-4790-98e1-775da7a23560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import trainers, ByteLevelBPETokenizer, Tokenizer\n",
    "from transformers import T5TokenizerFast\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "476d001d-c73d-4a22-86b2-601314d1e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf \"./model_artifacts/pretrained_tokenizer_generic/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7c484b0-bab9-4112-ab84-084cbbdd97b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(dataset, batch_size=1000):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"from\"]\n",
    "\n",
    "\n",
    "def build_fast_tokenizer(dataset_path):\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "    dataset = load_dataset(\"json\", data_files=[dataset_path], split=\"train\")\n",
    "    tokenizer.train_from_iterator(\n",
    "        batch_iterator(dataset),\n",
    "        vocab_size=50265,\n",
    "        min_frequency=2,\n",
    "        special_tokens=[\n",
    "            \"<s>\",\n",
    "            \"<pad>\",\n",
    "            \"</s>\",\n",
    "            \"<unk>\",\n",
    "            \"<mask>\",\n",
    "        ],\n",
    "    )\n",
    "    return T5TokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3916ccdb-0803-4a48-9bc2-fe4dd51c320a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = build_fast_tokenizer(\"./datasets/correction_train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2e41cc0-d216-4e73-8118-2eb888d9eb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model_artifacts/pretrained_tokenizer_generic/tokenizer_config.json',\n",
       " './model_artifacts/pretrained_tokenizer_generic/special_tokens_map.json',\n",
       " './model_artifacts/pretrained_tokenizer_generic/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./model_artifacts/pretrained_tokenizer_generic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88656bcf-5c94-48b0-bfff-94262c728b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "490108bb-6271-40b8-beeb-23fd16330e8a",
   "metadata": {},
   "source": [
    "# Alt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b232a2-e27c-4e1e-be20-b164eb0f6c8a",
   "metadata": {},
   "source": [
    "ref: https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#train-tokenizer-2\n",
    "```python\n",
    "from utils.tokenizer import SentencePieceUnigramTokenizer\n",
    "\n",
    "vocab_size = 32_0\n",
    "input_sentence_size = None\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=[\"./datasets/correction_train.jsonl\"], split=\"train\")\n",
    "\n",
    "tokenizer = SentencePieceUnigramTokenizer(unk_token=\"<unk>\", eos_token=\"</s>\", pad_token=\"<pad>\")\n",
    "\n",
    "\n",
    "def batch_iterator(input_sentence_size=None):\n",
    "    if input_sentence_size is None:\n",
    "        input_sentence_size = len(dataset)\n",
    "    batch_length = 100\n",
    "    for i in range(0, input_sentence_size, batch_length):\n",
    "        yield dataset[i: i + batch_length][\"from\"]\n",
    "\n",
    "\n",
    "tokenizer.train_from_iterator(\n",
    "    iterator=batch_iterator(input_sentence_size=input_sentence_size),\n",
    "    vocab_size=vocab_size,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "# Save files to disk\n",
    "tokenizer.save(\"./hudai/tokenizer.json\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4277e1fc-43fa-44ba-aed1-89432e5b6ee1",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dd87f30-743b-4751-b0bc-043b3462e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f5d41ed-3ce6-4789-8292-0397c942b13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5TokenizerFast.from_pretrained(\n",
    "    \"./model_artifacts/pretrained_tokenizer_generic/\"\n",
    ")\n",
    "# tokenizer = T5TokenizerFast.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8700789-3e7a-424a-ba7b-08b249011018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe465d54-faf1-4c68-b690-ff52f9e4f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\n",
    "    \"আমি বাংলায় গান গাই the quick brown fix jumps over the lazy dog THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG\",\n",
    "    return_tensors=\"pt\",\n",
    ").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52be0bbd-2a94-4386-a4ad-e4aa5d3e0686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  563,   268,   280,   359,   272,   264,   275,   323,   264,   269,\n",
       "           323,   264,   293,   963, 37177,  6844,  7445,  1004,  6147,  4260,\n",
       "          3310,  6612,  9920,   963,  1241,  5653,    93,  3020,    75,  1425,\n",
       "            44,    41,  8099,    57, 14804,    47,  1665,    54,    51,    59,\n",
       "            50,  2682,    51,    60,  4609,    57,    49,    52,    55,  3155,\n",
       "            58, 33097,  1425,    44,    41,  3744,    37,    62,    61,  1170,\n",
       "            51,    43]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dead9df6-f6a0-49df-897b-a8f838852f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['আমি বাংলায় গান গাই the quick brown fix jumps over the lazy dog THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898cff1-f893-4591-8307-486928a4c0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
