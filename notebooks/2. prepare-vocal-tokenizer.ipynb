{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0263a1a-6475-4403-9e99-61fe619e930c",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d0bac63-fa44-428c-970e-619604beb83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb8b7f95-4de2-4b4b-9484-a1a3f0061252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61477fa-6f2f-4341-87fe-886cf12767a4",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fb2d8ba-2d68-4a37-afc1-75e2cd2c7ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = json.load(open(\"./datasets/correction_train.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b47e0e6a-b46f-493c-a17a-89e4b57e186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_text = [d[\"from\"] for d in dataset]\n",
    "merged_text = \"\".join(merged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb1be5d-b0d6-490c-90b9-b2c6abec2d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./datasets/vocab_generic.txt\", \"w\") as file:\n",
    "    file.write(merged_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c6f5c2-eb71-4dd8-a954-091c26a6c584",
   "metadata": {},
   "source": [
    "# Build Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2058146-96af-473b-9039-5828ca9104a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TOKENIZERS_PARALLELISM=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da232755-c453-4b2b-9b93-a81853adcb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import trainers, ByteLevelBPETokenizer\n",
    "from transformers import T5TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "525844fa-3a9c-43a3-b8dc-2ceee846edcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fast_bert_tokenizer(files, max_vocab_size):\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "    trainer = trainers.BpeTrainer(show_progress=True)\n",
    "    tokenizer.train(files=files)\n",
    "    return T5TokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fae5acd3-0f5c-41bb-9326-c8cb8cdd14d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = build_fast_bert_tokenizer(files=[\"./datasets/vocab_generic.txt\"], max_vocab_size=40000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "476d001d-c73d-4a22-86b2-601314d1e3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf \"./model_artifacts/pretrained_tokenizer_generic/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2e41cc0-d216-4e73-8118-2eb888d9eb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model_artifacts/pretrained_tokenizer_generic/tokenizer_config.json',\n",
       " './model_artifacts/pretrained_tokenizer_generic/special_tokens_map.json',\n",
       " './model_artifacts/pretrained_tokenizer_generic/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./model_artifacts/pretrained_tokenizer_generic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4277e1fc-43fa-44ba-aed1-89432e5b6ee1",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dd87f30-743b-4751-b0bc-043b3462e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f5d41ed-3ce6-4789-8292-0397c942b13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5TokenizerFast.from_pretrained(\"./model_artifacts/pretrained_tokenizer_generic/\")\n",
    "# tokenizer = T5TokenizerFast.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8700789-3e7a-424a-ba7b-08b249011018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe465d54-faf1-4c68-b690-ff52f9e4f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"আমি বাংলায় গান গাই the quick brown fix jumps over the lazy dog THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG\", return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52be0bbd-2a94-4386-a4ad-e4aa5d3e0686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  559,   263,   275,   354,   267,   259,   270,   318,   259,   264,\n",
       "           318,   259,   288,   958,  4401, 10391,  6914,  7516,  1003,  6197,\n",
       "          4280,  3325,  6678, 10043,   958,  1241,  5698,    88,  3025,    70,\n",
       "          1427,    39,    36,  8190,    52, 15031,    42,  1667,    49,    46,\n",
       "            54,    45,  2696,    46,    55,  4639,    52,    44,    47,    50,\n",
       "          3173,    53,    36,    49,  1427,    39,    36,  3766,    32,    57,\n",
       "            56,  1168,    46,    38]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dead9df6-f6a0-49df-897b-a8f838852f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['আমি বাংলায় গান গাই the quick brown fix jumps over the lazy dog THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898cff1-f893-4591-8307-486928a4c0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
